# GitHub Repository Review: transit-delay-prediction

**Repository:** <https://github.com/Diennhutvo/transit-delay-prediction>  
**Owner:** Diennhutvo  
**Last Updated:** January 16, 2026  
**Total Commits:** 5

---

## üéØ Executive Summary

**Great news!** Your team has already made significant progress on the data infrastructure:

‚úÖ **GTFS Static data** is included in the repository  
‚úÖ **GTFS Realtime API integration** is working  
‚úÖ **Data collection scripts** are functional  
‚úÖ **TransLink API credentials** are available  
‚úÖ **Data parsing to CSV** is implemented

**This solves the critical gap identified earlier** - you now have access to realtime delay data!

---

## üìÅ Repository Structure

```
transit-delay-prediction/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/gtfs_rt/trip_updates/    # Raw realtime data (.pb files)
‚îÇ   ‚îî‚îÄ‚îÄ interim/gtfs_rt/              # Parsed CSV files
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ test_gtfs_rt_trip_updates.py  # Download realtime data
‚îÇ   ‚îî‚îÄ‚îÄ parse_trip_updates_to_csv.py  # Parse to CSV format
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## üîß What's Been Implemented

### 1. **GTFS Realtime Data Collection** ‚úÖ

**File:** `scripts/test_gtfs_rt_trip_updates.py`

**What it does:**

- Connects to TransLink GTFS Realtime API
- Downloads trip updates (actual arrival/departure times and delays)
- Saves raw Protocol Buffer (.pb) files to `data/raw/gtfs_rt/trip_updates/`
- Validates the feed and shows sample data

**Key Features:**

- Uses environment variables for API key security (`.env` file)
- Error handling for API requests
- Timestamped file naming for tracking
- Prints diagnostic info (feed timestamp, number of entities)

**API Credentials Provided:**

- Username: `mban2026`
- Password: `Mban2026!`
- API Key: `e25jm8i2vh3ZHGgLRndd`

---

### 2. **Data Parsing to CSV** ‚úÖ

**File:** `scripts/parse_trip_updates_to_csv.py`

**What it does:**

- Reads the latest `.pb` file from raw data
- Parses GTFS Realtime Protocol Buffer format
- Extracts key fields for each stop on each trip
- **Creates the outcome variable:** `delay_15plus` (True if delay ‚â• 15 minutes)
- Saves to CSV in `data/interim/gtfs_rt/`

**Extracted Fields:**

- `feed_timestamp` - When data was collected
- `trip_id` - Links to GTFS static trips
- `route_id` - Which route
- `stop_id` - Which stop
- `stop_sequence` - Order of stops
- `delay_sec` - Delay in seconds
- `delay_min` - Delay in minutes (calculated)
- `delay_15plus` - **Binary outcome variable** (True/False)
- `arrival_time_epoch` - Predicted arrival time
- `departure_time_epoch` - Predicted departure time

**This is exactly what you need for your supervised learning problem!**

---

### 3. **Setup Instructions** ‚úÖ

The README provides clear setup steps:

1. Clone the repository
2. Create virtual environment
3. Install dependencies (requires `requests`, `pandas`, `python-dotenv`, `gtfs-realtime-bindings`)
4. Add API key to `.env` file
5. Run scripts to collect and parse data

---

## üéØ How This Solves Your Project Needs

### For Question 6 (Plot Distributions)

**You can now plot:**

‚úÖ **Delay distribution** - Histogram of `delay_min`  
‚úÖ **Outcome variable** - Bar chart of `delay_15plus` (class balance)  
‚úÖ **Delays by route** - Which routes have most delays  
‚úÖ **Delays by time** - When delays occur (need to collect data over time)  
‚úÖ **Delays by stop** - Which stops are problematic

**Next steps:**

1. Run the data collection script multiple times (or set up automated collection)
2. Accumulate data over several days/weeks
3. Merge with GTFS static data to add temporal features (hour, day of week)
4. Create visualizations

---

### For Question 7 (Baseline Model)

**You have the outcome variable!**

The parsed CSV includes `delay_15plus` which is your binary classification target.

**Baseline approaches you can implement:**

- Always predict "no delay" (majority class)
- Predict delay based on historical route performance
- Time-based rules (predict delay during rush hour)

---

### For Question 8 (Logistic Regression)

**You have the foundation!**

**Current data provides:**

- Outcome: `delay_15plus`
- Features: `route_id`, `stop_id`, `stop_sequence`

**To enhance, you need to:**

1. Merge with GTFS static data to get:
   - Scheduled arrival times
   - Hour of day, day of week
   - Route type, direction
2. Engineer features:
   - Time-based: hour, is_rush_hour, day_of_week
   - Route-based: historical delay rate per route
   - Stop-based: historical delay rate per stop
   - **Bus bunching:** Calculate headway from vehicle positions
3. Train logistic regression on merged dataset

---

## üìä Data Collection Status

### What You Have

- ‚úÖ Scripts to collect realtime data
- ‚úÖ Scripts to parse data to CSV
- ‚úÖ API credentials
- ‚úÖ Outcome variable creation

### What You Need

- ‚ö†Ô∏è **Historical data collection** - Run scripts regularly to build dataset
- ‚ö†Ô∏è **Merge with GTFS static** - Combine realtime delays with scheduled times
- ‚ö†Ô∏è **Feature engineering** - Add temporal and route-based features
- ‚ö†Ô∏è **Vehicle positions** - For bus bunching analysis (separate API endpoint)

---

## üöÄ Recommended Next Steps

### Immediate (This Week)

1. **Clone the repository locally**

   ```bash
   cd "/Users/rohanjasani/Desktop/2. Business Application/Group assignment"
   git clone https://github.com/Diennhutvo/transit-delay-prediction
   cd transit-delay-prediction
   ```

2. **Set up environment**

   ```bash
   python -m venv .venv
   source .venv/bin/activate  # macOS
   pip install requests pandas python-dotenv gtfs-realtime-bindings
   ```

3. **Create .env file**

   ```bash
   echo "TRANSLINK_API_KEY=e25jm8i2vh3ZHGgLRndd" > .env
   ```

4. **Test the scripts**

   ```bash
   python scripts/test_gtfs_rt_trip_updates.py
   python scripts/parse_trip_updates_to_csv.py
   ```

5. **Verify output**
   - Check `data/raw/gtfs_rt/trip_updates/` for `.pb` files
   - Check `data/interim/gtfs_rt/` for parsed CSV
   - Open CSV and inspect the data

---

### Short-term (Next 1-2 Weeks)

1. **Set up automated data collection**
   - Create a script that runs every 5-10 minutes
   - Use cron job (macOS) or Task Scheduler (Windows)
   - Collect data for at least 1-2 weeks

2. **Create data merging script**
   - Merge realtime CSV with GTFS static `stop_times.txt`
   - Match on `trip_id` and `stop_id`
   - Calculate actual vs scheduled times
   - Add temporal features (hour, day_of_week)

3. **Start Question 6 (Exploratory Analysis)**
   - Load merged dataset
   - Create distribution plots
   - Analyze patterns

---

### Medium-term (Week 3-4)

1. **Implement baseline model (Question 7)**
   - Simple rule-based predictor
   - Evaluate on test set

2. **Implement logistic regression (Question 8)**
    - Feature engineering
    - Train/test split
    - Model evaluation
    - Feature importance analysis

3. **Add bus bunching features (Optional)**
    - Use vehicle positions API endpoint
    - Calculate headway between buses
    - Test if bunching predicts delays

---

## üí° Code Improvements to Consider

### 1. **Automated Collection Script**

Create `scripts/collect_data_loop.py`:

```python
import time
import schedule
from pathlib import Path
import requests
import os
from dotenv import load_dotenv
from google.transit import gtfs_realtime_pb2

load_dotenv()
API_KEY = os.getenv("TRANSLINK_API_KEY")

def collect_trip_updates():
    """Collect and save trip updates"""
    url = f"https://gtfsapi.translink.ca/v3/gtfsrealtime?apikey={API_KEY}"
    raw_dir = Path("data/raw/gtfs_rt/trip_updates")
    raw_dir.mkdir(parents=True, exist_ok=True)
    
    ts = int(time.time())
    raw_path = raw_dir / f"trip_updates_{ts}.pb"
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        raw_path.write_bytes(response.content)
        print(f"[{time.ctime()}] Saved: {raw_path.name}")
    except Exception as e:
        print(f"[{time.ctime()}] Error: {e}")

# Run every 5 minutes
schedule.every(5).minutes.do(collect_trip_updates)

print("Starting data collection (every 5 minutes)...")
print("Press Ctrl+C to stop")

while True:
    schedule.run_pending()
    time.sleep(1)
```

---

### 2. **Merge Script**

Create `scripts/merge_static_realtime.py`:

```python
import pandas as pd
from pathlib import Path
from datetime import datetime

# Load GTFS static
stop_times = pd.read_csv("../google_transit/stop_times.txt")
trips = pd.read_csv("../google_transit/trips.txt")
calendar = pd.read_csv("../google_transit/calendar.txt")

# Load all realtime CSVs
rt_dir = Path("data/interim/gtfs_rt")
rt_files = sorted(rt_dir.glob("trip_updates_parsed_*.csv"))

rt_data = pd.concat([pd.read_csv(f) for f in rt_files], ignore_index=True)

# Merge
merged = rt_data.merge(
    stop_times[['trip_id', 'stop_id', 'arrival_time', 'stop_sequence']],
    on=['trip_id', 'stop_id'],
    how='left'
)

merged = merged.merge(
    trips[['trip_id', 'route_id', 'service_id', 'direction_id']],
    on='trip_id',
    how='left'
)

# Add temporal features
merged['timestamp'] = pd.to_datetime(merged['feed_timestamp'], unit='s')
merged['hour'] = merged['timestamp'].dt.hour
merged['day_of_week'] = merged['timestamp'].dt.dayofweek
merged['is_weekend'] = merged['day_of_week'] >= 5
merged['is_rush_hour'] = merged['hour'].isin([7, 8, 9, 16, 17, 18])

# Save
out_path = Path("data/processed/merged_delays.csv")
out_path.parent.mkdir(parents=True, exist_ok=True)
merged.to_csv(out_path, index=False)

print(f"Merged dataset saved: {out_path}")
print(f"Total records: {len(merged)}")
print(f"Trips with delays ‚â•15 min: {merged['delay_15plus'].sum()}")
```

---

## ‚ö†Ô∏è Important Notes

### 1. **API Rate Limits**

- Check TransLink API documentation for rate limits
- Don't poll too frequently (5-10 minutes is reasonable)
- Monitor your API usage

### 2. **Data Storage**

- `.pb` files are ignored by git (in `.gitignore`)
- Don't commit large data files to GitHub
- Consider using cloud storage for large datasets

### 3. **Data Privacy**

- This is public transit data (no privacy concerns)
- API key is in `.env` (not committed to git)
- Keep credentials secure

### 4. **Collaboration**

- Coordinate with Jay on data collection
- One person can run collection, share CSV files
- Use git for code, share data via cloud storage

---

## üìà Expected Timeline

| Week | Activity | Deliverable |
|------|----------|-------------|
| **Week 1** | Set up repo, test scripts, start collection | Working data pipeline |
| **Week 2** | Continue collection, merge data, Q6 plots | Exploratory analysis |
| **Week 3** | Q7 baseline, Q8 logistic regression | Initial models |
| **Week 4** | Refine models, interpretation, documentation | Complete analysis |

---

## üéâ Summary

**Your team is in great shape!** The hardest part (getting realtime data) is already solved. You have:

‚úÖ Working API integration  
‚úÖ Data collection scripts  
‚úÖ Parsing to CSV with outcome variable  
‚úÖ Clear path forward for Questions 6-8

**Next actions:**

1. Clone the repo locally
2. Run the scripts to verify they work
3. Start collecting data (run manually or set up automation)
4. Begin exploratory analysis for Question 6

You're well-positioned to complete the initial analysis section. Good luck! üöÄ
